{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from faiss-cpu) (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (4.48.2)\n",
      "Requirement already satisfied: sentence-transformers in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (3.4.1)\n",
      "Requirement already satisfied: filelock in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (2.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (4.13.0)\n",
      "Requirement already satisfied: scrapy in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (2.12.0)\n",
      "Requirement already satisfied: fastapi in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (0.115.8)\n",
      "Requirement already satisfied: uvicorn in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (0.34.0)\n",
      "Requirement already satisfied: requests in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: Twisted>=21.7.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (24.11.0)\n",
      "Requirement already satisfied: cryptography>=37.0.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (44.0.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (1.3.2)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (1.10.0)\n",
      "Requirement already satisfied: pyOpenSSL>=22.0.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (25.0.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (1.7.0)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (24.2.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (2.3.1)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (7.2)\n",
      "Requirement already satisfied: protego>=0.1.15 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (0.4.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (0.11.0)\n",
      "Requirement already satisfied: packaging in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (24.2)\n",
      "Requirement already satisfied: tldextract in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (5.1.3)\n",
      "Requirement already satisfied: lxml>=4.6.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (5.3.0)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (0.7.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from scrapy) (2.0.7)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from fastapi) (0.45.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from fastapi) (2.10.6)\n",
      "Requirement already satisfied: click>=7.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from uvicorn) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from service-identity>=18.1.0->scrapy) (25.1.0)\n",
      "Requirement already satisfied: pyasn1 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from starlette<0.46.0,>=0.40.0->fastapi) (4.8.0)\n",
      "Requirement already satisfied: automat>=24.8.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from Twisted>=21.7.0->scrapy) (24.8.1)\n",
      "Requirement already satisfied: constantly>=15.1 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from Twisted>=21.7.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from Twisted>=21.7.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=24.7.0 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from Twisted>=21.7.0->scrapy) (24.7.2)\n",
      "Requirement already satisfied: setuptools in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from zope.interface>=5.1.0->scrapy) (75.8.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from tldextract->scrapy) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from tldextract->scrapy) (3.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Requirement already satisfied: pycparser in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
      "Requirement already satisfied: tomli in /Users/simarmehta/Library/Python/3.9/lib/python/site-packages (from incremental>=24.7.0->Twisted>=21.7.0->scrapy) (2.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu\n",
    "%pip install torch transformers sentence-transformers\n",
    "%pip install pandas numpy beautifulsoup4 scrapy fastapi uvicorn requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  restaurant_name menu_category   item_id       menu_item  \\\n",
      "0         20 spot      no proof  24932147  \"amaro\" spritz   \n",
      "1         20 spot      no proof  24932146   \"gin & tonic\"   \n",
      "2         20 spot      no proof  24932145   amalfi spritz   \n",
      "3         20 spot      no proof  24932145   amalfi spritz   \n",
      "5         20 spot      no proof  24932150    blood orange   \n",
      "\n",
      "          menu_description   ingredient_name  confidence  \\\n",
      "0  pathfinder amaro, tonic  pathfinder amaro        0.95   \n",
      "1                   lyre's               gin        0.80   \n",
      "2                   lyre's     amalfi spritz        0.95   \n",
      "3                   lyre's            lyre's        0.80   \n",
      "5           san pellegrino      blood orange        0.90   \n",
      "\n",
      "               categories      address1           city  zip_code country  \\\n",
      "0  New American|Wine Bars  3565 20th St  San Francisco   94110.0      US   \n",
      "1  New American|Wine Bars  3565 20th St  San Francisco   94110.0      US   \n",
      "2  New American|Wine Bars  3565 20th St  San Francisco   94110.0      US   \n",
      "3  New American|Wine Bars  3565 20th St  San Francisco   94110.0      US   \n",
      "5  New American|Wine Bars  3565 20th St  San Francisco   94110.0      US   \n",
      "\n",
      "  state  rating  review_count price  \n",
      "0    CA     4.3         270.0    $$  \n",
      "1    CA     4.3         270.0    $$  \n",
      "2    CA     4.3         270.0    $$  \n",
      "3    CA     4.3         270.0    $$  \n",
      "5    CA     4.3         270.0    $$  \n"
     ]
    }
   ],
   "source": [
    "def load_csv_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "csv_file_path = \"/Users/simarmehta/Downloads/Sample Ingredients File - MenuData Mission.csv\"\n",
    "data = load_csv_data(csv_file_path)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence from CSV:\n",
      "Restaurant: 20 spot. Menu Category: no proof. Item ID: 24932147. Menu Item: \"amaro\" spritz. Description: pathfinder amaro, tonic. Ingredient: pathfinder amaro. Confidence: 0.95. Categories: New American|Wine Bars. Address: 3565 20th St, San Francisco, CA 94110.0, US. Rating: 4.3 based on 270.0 reviews. Price: $$.\n",
      "Sentences have been saved to data_sentences.jsonl.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(csv_file_path)\n",
    "df.columns = [col.strip() for col in df.columns]\n",
    "\n",
    "def row_to_sentence(row):\n",
    "    sentence = (\n",
    "        f\"Restaurant: {row['restaurant_name']}. \"\n",
    "        f\"Menu Category: {row['menu_category']}. \"\n",
    "        f\"Item ID: {row['item_id']}. \"\n",
    "        f\"Menu Item: {row['menu_item']}. \"\n",
    "        f\"Description: {row['menu_description']}. \"\n",
    "        f\"Ingredient: {row['ingredient_name']}. \"\n",
    "        f\"Confidence: {row['confidence']}. \"\n",
    "        f\"Categories: {row['categories']}. \"\n",
    "        f\"Address: {row['address1']}, {row['city']}, {row['state']} {row['zip_code']}, {row['country']}. \"\n",
    "        f\"Rating: {row['rating']} based on {row['review_count']} reviews. \"\n",
    "        f\"Price: {row['price']}.\"\n",
    "    )\n",
    "    return sentence\n",
    "\n",
    "df['sentence'] = df.apply(row_to_sentence, axis=1)\n",
    "print(\"Example sentence from CSV:\")\n",
    "print(df['sentence'].iloc[0])\n",
    "\n",
    "output_file_path = 'data_sentences.jsonl'\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for _, row in df.iterrows():\n",
    "        record = {\n",
    "            \"item_id\": row['item_id'],\n",
    "            \"sentence\": row['sentence']\n",
    "        }\n",
    "        json.dump(record, outfile)\n",
    "        outfile.write(\"\\n\")\n",
    "print(f\"Sentences have been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping complete! Check your output file at: output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "input_file = \"data_sentences.jsonl\"  \n",
    "\n",
    "def extract_restaurant_name(sentence):\n",
    "    \"\"\"\n",
    "    Extracts the restaurant name from a sentence, for example:\n",
    "      'Restaurant: 20 spot. Menu Category: no proof...'\n",
    "    This function returns '20 spot'.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"Restaurant:\\s(.*?)\\.\", sentence)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"Unknown\"\n",
    "\n",
    "#  map {restaurant_name: [list_of_records]}\n",
    "grouped_data = defaultdict(list)\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue \n",
    "        \n",
    "        record = json.loads(line)\n",
    "        \n",
    "        sentence = record.get(\"sentence\", \"\")\n",
    "        restaurant_name = extract_restaurant_name(sentence)\n",
    "        \n",
    "        grouped_data[restaurant_name].append(record)\n",
    "\n",
    "grouped_data = dict(grouped_data)\n",
    "output_file_path=\"output.json\"\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(grouped_data, outfile, indent=2)\n",
    "\n",
    "print(f\"Grouping complete! Check your output file at: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index contains 52696 vectors.\n",
      "FAISS index saved to faiss_index.bin.\n",
      "Metadata mapping saved to metadata_mapping.pkl.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import faiss\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "json_file_path = 'output.json'        \n",
    "embedding_model_name = 'all-MiniLM-L6-v2'\n",
    "faiss_index_file = 'faiss_index.bin'\n",
    "\n",
    "\n",
    "data_records = []\n",
    "with open(json_file_path, 'r', encoding='utf-8') as infile:\n",
    "    grouped_data = json.load(infile)  \n",
    "    \n",
    "    for restaurant_name, records_list in grouped_data.items():\n",
    "        for record in records_list:\n",
    "            data_records.append(record)\n",
    "\n",
    "sentences = [record['sentence'] for record in data_records]\n",
    "\n",
    "model = SentenceTransformer(embedding_model_name)\n",
    "embeddings = model.encode(sentences, convert_to_numpy=True).astype('float32')\n",
    "\n",
    "embedding_dim = embeddings.shape[1]\n",
    "# faiss.normalize_L2(embeddings)  # Normalize embedddings for cosine similarity (another vector search mechanism to fetch simialr results)\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(embeddings)\n",
    "print(f\"FAISS index contains {index.ntotal} vectors.\")\n",
    "\n",
    "faiss.write_index(index, faiss_index_file)\n",
    "print(f\"FAISS index saved to {faiss_index_file}.\")\n",
    "\n",
    "with open('metadata_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(data_records, f)\n",
    "print(\"Metadata mapping saved to metadata_mapping.pkl.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_websites_to_json(urls, output_filename):\n",
    "    all_data = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=20)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            title = soup.title.string.strip() if soup.title else \"No Title Found\"\n",
    "            paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "            page_data = {\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"paragraphs\": paragraphs\n",
    "            }\n",
    "            all_data.append(page_data)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Could not scrape {url}: {str(e)}\")\n",
    "            all_data.append({\"url\": url, \"error\": str(e)})\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "urls_to_scrape = [\n",
    "    \"https://www.amenify.com/blog/a-culinary-world-tour-exploring-different-types-of-cuisines\",\n",
    "    \"https://en.wikipedia.org/wiki/History_of_pizza\",]\n",
    "output_file = \"multiples_scraped_data.json\"\n",
    "scrape_websites_to_json(urls_to_scrape, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:00<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 130 vectors.\n",
      "FAISS index saved to 'faiss_external.index'.\n",
      "Mapping saved to 'faiss_external_mapping.json'.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    json_filename = \"multiples_scraped_data.json\"\n",
    "    with open(json_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    all_paragraphs = []\n",
    "    metadata_list = []\n",
    "    for entry in data:\n",
    "        paragraphs = entry.get(\"paragraphs\", [])\n",
    "        url = entry.get(\"url\", \"\")\n",
    "        title = entry.get(\"title\", \"\")\n",
    "        for p in paragraphs:\n",
    "            all_paragraphs.append(p)\n",
    "            metadata_list.append((url, title, p))\n",
    "\n",
    "    if not all_paragraphs:\n",
    "        print(\"No paragraphs found in the JSON.\")\n",
    "        return\n",
    "\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = model.encode(all_paragraphs, show_progress_bar=True).astype(\"float32\")\n",
    "\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    print(f\"FAISS index created with {index.ntotal} vectors.\")\n",
    "\n",
    "    mapping = {}\n",
    "    for i, (url, title, paragraph) in enumerate(metadata_list):\n",
    "        mapping[i] = {\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"paragraph\": paragraph\n",
    "        }\n",
    "\n",
    "    faiss_index_filename = \"faiss_external.index\"\n",
    "    faiss.write_index(index, faiss_index_filename)\n",
    "    print(f\"FAISS index saved to '{faiss_index_filename}'.\")\n",
    "\n",
    "    mapping_filename = \"faiss_external_mapping.json\"\n",
    "    with open(mapping_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(mapping, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Mapping saved to '{mapping_filename}'.\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
